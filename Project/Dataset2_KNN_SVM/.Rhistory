library(foreach)
library(iterators)
library(parallel)
library(e1071)
library(caTools)
#after converting the cleaned data set after converting it into csv
datadf <- read.csv('/home/rajesh/cleaned_gaming.csv')
str(datadf)
#getting the necessary columns to train
datadf <- datadf[c(3,16,17,18)]
datadf_mat <- data.matrix(datadf)
#converting it into a matrix to know the correlation and for heat map
datadf_mat <- data.matrix(datadf)
heatmap(datadf_mat,scale = 'column')
corrplot(cor(datadf_mat), method="number")
library(corrplot)
corrplot(cor(datadf_mat), method="number")
# to divide the data set to 80:20 ratio for training and testing
sample <- sample.split(datadf$SWL_summary,SplitRatio = 0.8)
#assigning train and test set
train <- subset(datadf,sample==TRUE)
test <- subset(datadf,sample==FALSE)
#training
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
#start time
start.time<-proc.time()
tree <- rpart(SWL_summary ~.,data = train)
#ending
stop.time<-proc.time()
#run time of the model
run.time<-stop.time -start.time
print(run.time)
# to divide the data set to 80:20 ratio for training and testing
sample <- sample.split(datadf$SWL_T,SplitRatio = 0.8)
#assigning train and test set
train <- subset(datadf,sample==TRUE)
test <- subset(datadf,sample==FALSE)
#training
#dividing the load into 6 cluster to do parallel processing
cl <- makePSOCKcluster(6)
#training
#dividing the load into 6 cluster to do parallel processing
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
start.time<-proc.time()
tree <- rpart(SWL_T ~.,data = train)
#ending
stop.time<-proc.time()
#run time of the model
run.time<-stop.time -start.time
stopCluster(cl)
print(run.time)
tree.SWL_summary.predicted <- predict(tree,test,type = 'class')
tree.SWL_T.predicted
tree.SWL_T.predicted <- predict(tree,test,type = 'class')
tree.SWL.predicted <- predict(tree,test,type = 'class')
#after converting the cleaned data set after converting it into csv
datadf <- read.csv('/home/rajesh/cleaned_gaming.csv')
str(datadf)
#after converting the cleaned data set after converting it into csv
datadf <- read.csv('/home/rajesh/cleaned_gaming.csv')
str(datadf)
#getting the necessary columns to train
datadf <- datadf[c(3,16,17,19)]
#converting it into a matrix to know the correlation and for heat map
datadf_mat <- data.matrix(datadf)
heatmap(datadf_mat,scale = 'column')
corrplot(cor(datadf_mat), method="number")
#after converting the cleaned data set after converting it into csv
datadf <- read.csv('/home/rajesh/cleaned_gaming.csv')
str(datadf)
#getting the necessary columns to train
datadf <- datadf[c(3,16,18,19)]
str(datadf)
#converting it into a matrix to know the correlation and for heat map
datadf_mat <- data.matrix(datadf)
heatmap(datadf_mat,scale = 'column')
corrplot(cor(datadf_mat), method="number")
#after converting the cleaned data set after converting it into csv
datadf <- read.csv('/home/rajesh/cleaned_gaming.csv')
str(datadf)
#getting the necessary columns to train
datadf <- datadf[c(3,16,17,18)]
#converting it into a matrix to know the correlation and for heat map
datadf_mat <- data.matrix(datadf)
heatmap(datadf_mat,scale = 'column')
corrplot(cor(datadf_mat), method="number")
#split dataset
set.seed(123)
# to divide the data set to 80:20 ratio for training and testing
sample <- sample.split(datadf$SWL_T,SplitRatio = 0.8)
#assigning train and test set
train <- subset(datadf,sample==TRUE)
test <- subset(datadf,sample==FALSE)
#training
#dividing the load into 6 cluster to do parallel processing
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
#start time
start.time<-proc.time()
tree <- rpart(SWL_T ~.,data = train)
#ending
stop.time<-proc.time()
#run time of the model
run.time<-stop.time -start.time
print(run.time)
tree.SWL_T.predicted <- predict(tree,test,type = 'class')
tree.SWL_T.predicted <- predict(tree,test)
tree.SWL_T.predicted
prp(tree)
tree.SWL_T.predicted <- as.integer(tree.SWL_T.predicted)
tree.SWL_T.predicted
#converted to integer to get rid of decimal
tree.SWL_T.predicted <- round(tree.SWL_T.predicted)
tree.SWL_T.predicted
confusionMatrix(tree.SWL_T.predicted,test$SWL_T)
ACC <- sum(tree.SWL_T.predicted == test$SWL_T)/NROW(test)
ACC
confusionMatrix(table(tree.SWL_T.predicted,test$SWL_T))
#after converting the cleaned data set after converting it into csv
datadf <- read.csv('/home/rajesh/cleaned_gaming.csv')
str(datadf)
#getting the necessary columns to train
datadf <- datadf[c(3,7,9,16,17,18)]
str(datadf)
#converting it into a matrix to know the correlation and for heat map
datadf_mat <- data.matrix(datadf)
corrplot(cor(datadf_mat), method="number")
#after converting the cleaned data set after converting it into csv
datadf <- read.csv('/home/rajesh/cleaned_gaming.csv')
str(datadf)
#converting it into a matrix to know the correlation and for heat map
datadf_mat <- data.matrix(datadf)
corrplot(cor(datadf_mat), method="number")
#after converting the cleaned data set after converting it into csv
datadf <- read.csv('/home/rajesh/cleaned_gaming.csv')
#getting the necessary columns to train
datadf <- datadf[c(3,7,16,17,18)]
#converting it into a matrix to know the correlation and for heat map
datadf_mat <- data.matrix(datadf)
corrplot(cor(datadf_mat), method="number")
corrplot(cor(datadf_mat),col='black' method="number")
corrplot(cor(datadf_mat),col='black',method="number")
# to divide the data set to 80:20 ratio for training and testing
sample <- sample.split(datadf$SWL_T,SplitRatio = 0.8)
#assigning train and test set
train <- subset(datadf,sample==TRUE)
test <- subset(datadf,sample==FALSE)
#training
#dividing the load into 6 cluster to do parallel processing
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
heatmap(datadf_mat,scale = 'column')
#training
#dividing the load into 6 cluster to do parallel processing
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
#start time
start.time<-proc.time()
tree <- rpart(SWL_T ~.,data = train)
#ending
stop.time<-proc.time()
#run time of the model
run.time<-stop.time -start.time
print(run.time)
stopCluster(cl)
str(train)
tree.SWL_T.predicted <- predict(tree,test)
#converted to integer to get rid of decimal
tree.SWL_T.predicted <- round(tree.SWL_T.predicted)
#creating a decision tree
prp(tree)
# to divide the data set to 80:20 ratio for training and testing
sample <- sample.split(datadf$SWL_T,SplitRatio = 0.8)
#assigning train and test set
train <- subset(datadf,sample==TRUE)
test <- subset(datadf,sample==FALSE)
#training
#dividing the load into 6 cluster to do parallel processing
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
#start time
start.time<-proc.time()
tree <- rpart(SWL_T ~.,data = train)
#ending
stop.time<-proc.time()
#run time of the model
run.time<-stop.time -start.time
print(run.time)
stopCluster(cl)
str(train)
tree.SWL_T.predicted <- predict(tree,test)
#converted to integer to get rid of decimal
tree.SWL_T.predicted <- round(tree.SWL_T.predicted)
#creating a decision tree
prp(tree)
#calculating Accuracy
ACC <- sum(tree.SWL_T.predicted == test$SWL_T)/NROW(test)
ACC
#calculating Accuracy
ACC <-100*sum(tree.SWL_T.predicted == test$SWL_T)/NROW(test)
ACC
confusionMatrix(table(tree.SWL_T.predicted,test$SWL_T))
library(tidyr)
library("ggplot2")
library(caTools)
library(class)
library(gmodels)
library(corrplot)
library(caret)
election <- read.csv("/home/rajesh/Desktop/DMML_project/Datasets/2/election.csv")
str(election)
#deleting unncessary columns
election <- election[-1]
str(election)
#deleting unncessary columns
election <- election[-1]
#reading the cleaned data set
election <- read.csv("/home/rajesh/Desktop/DMML_project/Datasets/2/election.csv")
str(election)
#getting necessary columns
election <- election[c(4,5,6,7,14:22)]
election$county <- as.factor(election$county)
str(election)
election_mat <- data.matrix(election)
heatmap(election_mat,scale = 'column')
cor(election_mat, col = 'black',method = 'number')
cor(election_mat, col='black',method = 'number')
corrplot(cor(election_mat),col='black',method="number")
write.csv(election,"/home/rajesh/Desktop/DMML_project/Datasets/2/election_cleaned.csv")
election <- read.csv("/home/rajesh/Desktop/DMML_project/Datasets/2/election_cleaned.csv")
str(election)
election <- election[-1]
elect_mat <- data.matrix(election)
#outliers
boxplot(election[2:13],outline = FALSE)
str(election)
election$candidate <- as.factor(election$candidate)
#sampling
train_election<-election[1:2500, ]
test_election <- election[2501:5509, ]
str(election)
#sampling the data sets for training and testing
train_election<-election[1:2500, ]
test_election <- election[2501:5509, ]
election$candidate <- as.numeric(election$candidate)
#assigning dependent variables
election_train_labels <- election[1:2500, 1]
election_test_labels <- election[2501:5509, 1]
knnmodel <- knn(train = train_election,test = test_election,cl = election_train_labels,k = 1 )
election$candidate <- as.factor(election$candidate)
election <- read.csv("/home/rajesh/Desktop/DMML_project/Datasets/2/election_cleaned.csv")
election <- election[-1]
election$candidate <- as.factor(election$candidate)
#sampling the data sets for training and testing
train_election<-election[1:2500, ]
test_election <- election[2501:5509, ]
#assigning dependent variables
election_train_labels <- election[1:2500, 1]
election_test_labels <- election[2501:5509, 1]
knnmodel <- knn(train = train_election,test = test_election,cl = election_train_labels,k = 1 )
sapply(election , function(x) sum(is.na(x)))
#sampling the data sets for training and testing
train_election<-election[1:2500, ]
test_election <- election[2501:5509, ]
#assigning dependent variables
election_train_labels <- election[1:2500, 1]
election_test_labels <- election[2501:5509, 1]
knnmodel <- knn(train = train_election,test = test_election,cl = election_train_labels,k = 1 )
knnmodel <- knn(train = train_election,test = test_election,cl = election_train_labels,k = 65 )
library(tidyr)
library("ggplot2")
library(caTools)
library(class)
library(gmodels)
library(corrplot)
library(caret)
knnmodel <- knn(train = train_election,test = test_election,cl = election_train_labels,k = 65 )
#set working directory
library(Amelia)
setwd("/home/rajesh/Desktop/DMML_project/Datasets/2")
election_results <- read.csv("primary_results.csv")
county_facts <- read.csv("county_facts.csv")
#merge two data sets
merged <- merge(election_results,county_facts,by='fips')
#check the structure of data set
str(merged)
#plot of percentage of missing values
missmap(merged,main="missing in merged")
#finding the number of missing values in sapply
sapply(merged, function(x) sum(is.na(x)))
#conerting dataframe to csv file
write.csv(merged,"/home/rajesh/merged_cleaned_election_result.csv")
merged_to_cleaning <- read.csv("merged_cleaned_election_result.csv")
boxplot(merged_to_cleaning,outline = FALSE)
merged_to_cleaning <- merged_to_cleaning[-c(1,3,4,10,11)]
merged_to_cleaning <- merged_to_cleaning[-c(1)]
str(merged_to_cleaning)
merged_to_cleaning <- merged_to_cleaning[(merged_to_cleaning$candidate %in% c("Donald Trump","Hillary Clinton")),]
write.csv(merged_to_cleaning,"election.csv")
str(merged_to_cleaning)
merged_to_cleaning <- merged_to_cleaning[c(3:6,13:21)]
merged_to_cleaning <- merged_to_cleaning[(merged_to_cleaning$candidate %in% c("Donald Trump","Hillary Clinton")),]
write.csv(merged_to_cleaning,"election.csv")
#reading the cleaned data set
election <- read.csv("/home/rajesh/Desktop/DMML_project/Datasets/2/election.csv")
str(election)
election <- election[-1]
str(election)
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election <- as.data.frame(lapply(election[2:13], normalize))
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election <- as.data.frame(lapply(merged_to_cleaning[2:13], normalize))
election <- election[(election$candidate %in% c("Donald Trump","Hillary Clinton")),]
write.csv(merged_to_cleaning,"election.csv")
write.csv(election,"election.csv")
merged_to_cleaning <- read.csv("merged_cleaned_election_result.csv")
str(merged_to_cleaning)
merged_to_cleaning <- merged_to_cleaning[c(3:6,13:21)]
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election <- as.data.frame(lapply(merged_to_cleaning[2:13], normalize))
election <- election[(election$candidate %in% c("Donald Trump","Hillary Clinton")),]
write.csv(election,"election.csv")
merged_to_cleaning <- read.csv("merged_cleaned_election_result.csv")
str(merged_to_cleaning)
merged_to_cleaning <- merged_to_cleaning[c(7:9,12,19:27)]
str(merged_to_cleaning)
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election <- as.data.frame(lapply(merged_to_cleaning[2:13], normalize))
str(merged_to_cleaning)
str(election)
election <- election[(election$candidate %in% c("Donald Trump","Hillary Clinton")),]
str(election)
merged_to_cleaning <- read.csv("merged_cleaned_election_result.csv")
str(merged_to_cleaning)
merged_to_cleaning <- merged_to_cleaning[c(7:9,12,19:27)]
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election <- as.data.frame(lapply(merged_to_cleaning[2:13], normalize))
str(election)
merged_to_cleaning <- read.csv("merged_cleaned_election_result.csv")
str(merged_to_cleaning)
merged_to_cleaning <- merged_to_cleaning[c(7:9,12,19:27)]
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election <- as.data.frame(lapply(merged_to_cleaning[1:13], normalize))
merged_to_cleaning <- read.csv("merged_cleaned_election_result.csv")
str(merged_to_cleaning)
merged_to_cleaning$candidate <- as.factor(merged_to_cleaning$candidate)
merged_to_cleaning <- merged_to_cleaning[c(7:9,12,19:27)]
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election <- as.data.frame(lapply(merged_to_cleaning[1:13], normalize))
merged_to_cleaning <- read.csv("merged_cleaned_election_result.csv")
#removing unnecessary columns
merged_to_cleaning <- merged_to_cleaning[c(7:9,12,19:27)]
str(election)
str(merged_to_cleaning)
merged_to_cleaning <- merged_to_cleaning[(merged_to_cleaning$candidate %in% c("Donald Trump","Hillary Clinton")),]
write.csv(election,"election.csv")
#reading the cleaned data set
election <- read.csv("/home/rajesh/Desktop/DMML_project/Datasets/2/election.csv")
merged_to_cleaning <- read.csv("merged_cleaned_election_result.csv")
#removing unnecessary columns
merged_to_cleaning <- merged_to_cleaning[c(7:9,12,19:27)]
str(merged_to_cleaning)
#converted for knn and svm
write.csv(merged_to_cleaning,"election.csv")
#reading the cleaned data set
election <- read.csv("/home/rajesh/Desktop/DMML_project/Datasets/2/election.csv")
election <- election[-1]
election$candidate <- as.factor(election$candidate)
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election[2:13] <- as.data.frame(lapply(election[2:13], normalize))
str(election)
merged_to_cleaning <- read.csv("merged_cleaned_election_result.csv")
#removing unnecessary columns
merged_to_cleaning <- merged_to_cleaning[c(7:9,12,19:27)]
str(merged_to_cleaning)
merged_to_cleaning <- merged_to_cleaning[(merged_to_cleaning$candidate %in% c("Donald Trump","Hillary Clinton")),]
str(merged_to_cleaning)
#converted for knn and svm
write.csv(merged_to_cleaning,"election.csv")
#reading the cleaned data set
election <- read.csv("/home/rajesh/Desktop/DMML_project/Datasets/2/election.csv")
election <- election[-1]
election$candidate <- as.factor(election$candidate)
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election[2:13] <- as.data.frame(lapply(election[2:13], normalize))
str(election)
election_mat <- data.matrix(election)
heatmap(election_mat,scale = 'column')
corrplot(cor(election_mat),col='black',method="number")
corrplot(cor(election_mat),col='black',method="number")
corrplot(cor(election_mat),col='black',method="number")
#sampling the data sets for training and testing
train_election<-election[1:2500, ]
test_election <- election[2501:5509, ]
#assigning dependent variables
election_train_labels <- election[1:2500, 1]
election_test_labels <- election[2501:5509, 1]
knnmodel <- knn(train = train_election,test = test_election,cl = election_train_labels,k = 65 )
ACC <- 100*sum(election_test_labels == knnmodel)/NROW(election_test_labels)
election$candidate <- as.numeric(election$candidate)
#sampling the data sets for training and testing
train_election<-election[1:2500, ]
test_election <- election[2501:5509, ]
#assigning dependent variables
election_train_labels <- election[1:2500, 1]
election_test_labels <- election[2501:5509, 1]
knnmodel <- knn(train = train_election,test = test_election,cl = election_train_labels,k = 65 )
ACC <- 100*sum(election_test_labels == knnmodel)/NROW(election_test_labels)
ACC
table(knnmodel,election_test_labels)
confusionMatrix(testpedict,test_election$candidate)
testpedict <- predict(knnmodel, newdata = test_election$candidate)
test_election$candidate <- as.factor(test_election$candidate)
testpedict <- predict(knnmodel, newdata = test_election$candidate)
table(knnmodel,election_test_labels)
testpedict <- predict(knnmodel, newdata = election_test_labels)
confusionMatrix(table(testpedict,election_test_labels))
confusionMatrix(table(knnmodel,election_test_labels))
#election svm train model
# loading the cleaned dataset
election_svm <- read.csv("/home/rajesh/Desktop/DMML_project/Datasets/2/election.csv")
election_svm$candidate <- as.factor(election_svm$candidate)
str(election_svm)
election_svm <- election_svm[-``]
election_svm <- election_svm[-1]
str(election_svm)
#normalizing the values in the dataset
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election[2:13] <- as.data.frame(lapply(election[2:13], normalize))
str(election_svm)
#normalizing the values in the dataset
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
election_svm[2:13] <- as.data.frame(lapply(election_svm[2:13], normalize))
str(election_svm)
str(election_svm)
train_election <- election_svm[1:4000, ]
test_election <- election_svm[4001:5509, ]
#training model
svm_model <- svm(candidate~.,
data = train_election,
type ='C-classification',
kernel = 'linear',
scale = FALSE)
#get summary of the model
summary(svm_model)
#show support vectors
test_predict <- predict(svm_model,newdata = test_election)
confusionMatrix(table(test_predict,test_election$candidate))
#tune grid for changing the ways to tune the learnig
grid <- expand.grid(C = c(0,0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
svm_model_linear_grid <- train(candidate~.,
data = train_election,
method = 'svmLinear',
preProcess = c("center","scale"),
tuneGrid = grid,
tuneLength = 10)
svm_model_linear_grid <- train(candidate~.,
data = train_election,
method = 'svmLinear',
preProcess = c("center","scale"),
tuneGrid = grid,
tuneLength = 10)
svm_model_linear_grid
#tune grid for changing the ways to tune the learnig
grid <- expand.grid(C = c(0,0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
svm_model_linear_grid <- train(candidate~.,
data = train_election,
method = 'svmLinear',
preProcess = c("center","scale"),
tuneGrid = grid,
tuneLength = 10)
#tune grid for changing the ways to tune the learnig
grid <- expand.grid(C = c(0,0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
svm_model_linear_grid <- train(candidate~.,
data = train_election,
method = 'svmLinear',
preProcess = c("center","scale"),
tuneGrid = grid,
tuneLength = 10)
str(train_election)
#tune grid for changing the ways to tune the learnig
grid <- expand.grid(C = c(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
svm_model_linear_grid <- train(candidate~.,
data = train_election,
method = 'svmLinear',
preProcess = c("center","scale"),
tuneGrid = grid,
tuneLength = 10)
